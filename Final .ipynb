{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Titanic (Regression)\n",
    "\n",
    "https://www.kaggle.com/c/titanic/overview\n",
    "\n",
    "출력을 csv파일로 저장하여 함께 제출\n",
    "\n",
    "submit predictions에 테스트해 볼 것!\n",
    "\n",
    "어떠한 알고리즘을 사용하여도 무방함\n",
    "\n",
    "참조: https://www.kaggle.com/alexisbcook/getting-started-with-titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./titanic/train.csv')\n",
    "test_df = pd.read_csv('./titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 자료 int, float 변수 통계치\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불필요하다고 생각된 기존 데이터에서 지우기 식별번호, 이름, 티켓 구별 번호,티켓요금, 탑승항구를 기존 배열에서 제거\n",
    "train_df = train_df.drop(['PassengerId', 'Name', 'Ticket',\"Fare\",\"Embarked\"], axis=1)\n",
    "test_df = test_df.drop(['Name','Ticket',\"Fare\",\"Embarked\"], axis=1)\n",
    "\n",
    "#정보에 nan이 너무 많아 정확한 분석이 불가능해서 제외\n",
    "train_df = train_df.drop(['Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass 데이터는 1등급 2등급 3등급을 나타내며 \n",
    "#수치적으로 아무 의미 없는 데이터이므로 경우의 수로 판단해 3개의 경우로 나눈다\n",
    "\n",
    "\n",
    "#Pclass를 3개의 경우로 나누기\n",
    "pclass_train_dummies = pd.get_dummies(train_df['Pclass'])#학습\n",
    "pclass_test_dummies = pd.get_dummies(test_df['Pclass'])#테스트\n",
    "\n",
    "#변경전 내용 기존 데이터에서 제거\n",
    "train_df.drop(['Pclass'], axis=1, inplace=True)\n",
    "test_df.drop(['Pclass'], axis=1, inplace=True)\n",
    "\n",
    "#변경된 내용 기존 데이터에 추가\n",
    "train_df = train_df.join(pclass_train_dummies)\n",
    "test_df = test_df.join(pclass_test_dummies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#성별을 계산하기 위해 성별을 경우로 분리\n",
    "\n",
    "#성별을 2개의 경우로 나누기\n",
    "sex_train_dummies = pd.get_dummies(train_df['Sex'])\n",
    "sex_test_dummies = pd.get_dummies(test_df['Sex'])\n",
    "\n",
    "#나눈 경우를 구별하기 위해 분류에 이름 붙이기\n",
    "sex_train_dummies.columns = ['Female', 'Male']\n",
    "sex_test_dummies.columns = ['Female', 'Male']\n",
    "\n",
    "#변경전  성별테이터 제거\n",
    "train_df.drop(['Sex'], axis=1, inplace=True)\n",
    "test_df.drop(['Sex'], axis=1, inplace=True)\n",
    "\n",
    "#변경 된 데이터 추가\n",
    "train_df = train_df.join(sex_train_dummies)\n",
    "test_df = test_df.join(sex_test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나이값이 없는 경우가 있는데 nan데이터가 들어가면 함수가 고장나므로\n",
    "#나이의 평균값을 집어 넣기로 함\n",
    "\n",
    "#나이 nan값에 평균값 집어넣기\n",
    "train_df[\"Age\"].fillna(train_df[\"Age\"].mean() , inplace=True)\n",
    "test_df[\"Age\"].fillna(train_df[\"Age\"].mean() , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#편의를 위한 데이터 분리\n",
    "\n",
    "#기존 데이터에서 정답 드랍\n",
    "X_train = train_df.drop(\"Survived\",axis=1)\n",
    "#정답 데이터 제작 \n",
    "Y_train = train_df[\"Survived\"]\n",
    "#정답 확인을 위해 ID가 필요는 하지만 인풋으로 넣을 필요는 없어 인풋에서만 제거한 데이터 파일 제작\n",
    "X_test  = test_df.drop(\"PassengerId\",axis=1).copy()\n",
    "\n",
    "\n",
    "Y_train=np.array(Y_train,ndmin=2)#형식 맞추기1. 차원 수 맞추기\n",
    "Y_train=Y_train.T#형식 맞춰주기2. 배열 형태 맞추기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#정규화 데이터 :중요도를 몰라 수치적 크기가 의미가 없다고 판단\n",
    "X_train=minmax_scale(X_train)\n",
    "Y_train=minmax_scale(Y_train)\n",
    "X_test=minmax_scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "#torch에서 지원하는 형식으로 맞춰주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([891, 8])\n",
      "torch.Size([891, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "#데이터 크기 확인 인풋 크기 정할때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "din, dh, dout = 8,24,1#입력 히든 아웃\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(din,dh),  # 인풋-히든\n",
    "    torch.nn.Sigmoid(),       #레이어에서 사용할 함수 Sigmoid\n",
    "    torch.nn.Linear(dh,dout), # 히든-아웃\n",
    "    torch.nn.Sigmoid()        #레이어에서 사용할 함수 Sigmoid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([891, 1])\n"
     ]
    }
   ],
   "source": [
    "print(model(X_train).shape)#데이터 양식 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3 #학습강도\n",
    "criterion = torch.nn.MSELoss()#손실 함수 MSELoss 사용\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)#Optimizer 로 Adam 사용\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t= 0 accu= 0.6161616444587708\n",
      "t= 1000 accu= 0.8092031478881836\n",
      "t= 2000 accu= 0.8125701546669006\n",
      "t= 3000 accu= 0.8181818127632141\n",
      "t= 4000 accu= 0.8193041682243347\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 4500# 학습 횟수\n",
    "for t in range(nb_epochs+1):\n",
    "    y_pred = model(X_train)#모델이 예측한 값을 저장\n",
    "    loss = criterion(y_pred,Y_train)#오차평균 저장\n",
    "    optimizer.zero_grad() #미분값 초기화 안하면 값 누적\n",
    "    loss.backward() # 역전파\n",
    "    optimizer.step() # update weights and biases\n",
    "    if t % 1000 == 0:#그냥 출력문 1000번마다 출력\n",
    "        prediction = model(X_train) >= torch.FloatTensor([0.5])#답결정 0.5보다 크면 참 작으면 거짓\n",
    "        corrects = (prediction == Y_train)#데이터 부울 값으로 저장\n",
    "        accuracy = corrects.sum().float() / float( Y_train.size(0) )#정확도 게산\n",
    "        print( \"t=\", t, \"accu=\", accuracy.item())#t 시행횟수 accu 정확도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv에 넣을 배열 만들기\n",
    "#양식에 맞게 데이터를 구성 부울 값이 1과0으로 인식할 줄 알았으나 참과 거짓으로 인식해서 부울 값 기반으로 숫자 배열을 다시생성\n",
    "prediction = model(X_test) >= torch.FloatTensor([0.5])#답결정\n",
    "fff = [0 for i in range(418)]\n",
    "for i in range(418):\n",
    "    if( prediction[i]):#값이 참이면\n",
    "        fff[i]=1#생존\n",
    "    else:\n",
    "        fff[i]=0#사망\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([418, 1])\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)#데이터 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": fff\n",
    "    })\n",
    "submission.to_csv('titanic.csv', index=False)\n",
    "#csv파일변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅이 언제나는지 알기가 어려워서 여러번 실험해봤다\n",
    "\n",
    "3000번~4500번 사이에서 학습을 하면 이후로 결과가 어느정도 수렴해 정확도가\n",
    "78% 근처로 나온다\n",
    "\n",
    "노드수는 바꿔 보았지만 정확도에 큰 영향을 주지는 못했다. \n",
    "노드수를 늘리는 것보다도 오버피팅을 잡는게 정확도 향상에 도움이 되었다.\n",
    "\n",
    "최대정확도는 0.78947이였고 오버피팅 직전에 운좋게 걸린 값 같다.\n",
    "\n",
    "가격과 승선항을 제거해도 정확도에 큰 하강폭이 없는 것으로 볼 때 큰 의미가 있는 요소는 아닌 것으로 보인다.\n",
    "\n",
    "같은 모델에서도 오버피팅 유무에 따라 정확도 차이가 크게 나는데\n",
    "오버피팅으로 0.67942까지 정확도가 떨어졌었다\n",
    "오버피팅을 잡는게 정확도 향상에 큰 도움이 되는 것으로 보인다.\n",
    "\n",
    "다른 모델도 만들어보았으나 기존 Adam, Sigmoid를 사용한 모델보다 정확도가 평균적으로 조금 낮거나 많이 떨어져 버렸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image classification with CNN\n",
    "\n",
    "cnn을 이용하여 다음 이미지를 분류하여라.\n",
    "\n",
    "인풋 데이터는 rgb 정보를 가지고 있다.\n",
    "\n",
    "코드, 결과, 보고서를 함께 제출하여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "classes = trainset.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24283f22280>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images=train_loader.dataset[1][0]\n",
    "plt.imshow(images.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#GPU초기셋팅\n",
    "\n",
    "# CUDA 기기가 존재한다면, 아래 코드가 CUDA 장치를 출력합니다:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(3, 600, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(600, 320, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=8000, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = nn.Conv2d(3, 600, 3)#RGB값이라 인 체널이 3, 필터수 , 필터에 한픽셀에 들어갈 정보 크기(Kernel Size)\n",
    "        #결과 값=데이터 행 크기-(Kernel Size-1)\n",
    "        #결과값*결과값*데이터 층 수*필터수\n",
    "        self.pool = nn.MaxPool2d(2, 2)#결과물 크기 반(1/2)으로 줄이기\n",
    "        #((결과 값)/2:나머지 버림)*((결과 값)/2)*데이터 층 수*필터수\n",
    "        #self.conv2 = nn.Conv2d(600, 320, 3)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 600, 5)\n",
    "    \n",
    "        self.conv2 = nn.Conv2d(600, 320, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(320 * 5 * 5, 120)#인풋320*5*5  아웃120\n",
    "        self.fc2 = nn.Linear(120, 84)#인풋120  아웃84\n",
    "        self.fc3 = nn.Linear(84, 10)#인풋84  아웃10\n",
    "\n",
    "    def forward(self, x):\n",
    "        #입력 4*32*32 (RGP+밝기=4)\n",
    "        x = self.pool(F.relu(self.conv1(x)))#F.relu와 nn.relu는 거의 같으며 호출방식의 차이다.\n",
    "        #1. self.conv1(x) 처리 [32-(5-1)=28]*[28]*4*600=28*28*600*4\n",
    "        #2. relu() 사용\n",
    "        #3. self.pool() 처리 [28/2=14]*[14]*4*600=14*14*600*4\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #1. self.conv1(x) 처리 [14-(5-1)=10]*[10]*4*320=10*10*320*4\n",
    "        #2. relu() 사용\n",
    "        #3. self.pool() 처리 [10/2]*[5]*4*600=5*5*320*4 :4는 층수\n",
    "    \n",
    "        \n",
    "        x = torch.flatten(x, 1) # 배치를 제외한 모든 차원을 평탄화(flatten) \n",
    "        x = F.relu(self.fc1(x)) #fc1신경망에서 relu 사용\n",
    "        x = F.relu(self.fc2(x)) #fc2신경망에서 relu 사용\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)#GPU셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate=0.001\n",
    "criterion = nn.CrossEntropyLoss()#손실 함수 CrossEntropyLoss 사용\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)#최적화 SGD 사용 Ir:학습속도 momentum 관성 이동방향으로 \n",
    "#optimizer = optim.RMSprop(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2000 accu= 0.1612\n",
      "1 4000 accu= 0.2555\n",
      "1 6000 accu= 0.3113\n",
      "1 8000 accu= 0.3641\n",
      "1 10000 accu= 0.4055\n",
      "1 12000 accu= 0.4268\n",
      "2 2000 accu= 0.4372\n",
      "2 4000 accu= 0.4597\n",
      "2 6000 accu= 0.4716\n",
      "2 8000 accu= 0.4971\n",
      "2 10000 accu= 0.4852\n",
      "2 12000 accu= 0.502\n",
      "3 2000 accu= 0.5192\n",
      "3 4000 accu= 0.5341\n",
      "3 6000 accu= 0.5342\n",
      "3 8000 accu= 0.5559\n",
      "3 10000 accu= 0.5511\n",
      "3 12000 accu= 0.5811\n",
      "4 2000 accu= 0.5992\n",
      "4 4000 accu= 0.6074\n",
      "4 6000 accu= 0.5911\n",
      "4 8000 accu= 0.6143\n",
      "4 10000 accu= 0.6167\n",
      "4 12000 accu= 0.6047\n",
      "5 2000 accu= 0.6052\n",
      "5 4000 accu= 0.6323\n",
      "5 6000 accu= 0.6381\n",
      "5 8000 accu= 0.6431\n",
      "5 10000 accu= 0.6438\n",
      "5 12000 accu= 0.6554\n",
      "6 2000 accu= 0.6688\n",
      "6 4000 accu= 0.6614\n",
      "6 6000 accu= 0.6673\n",
      "6 8000 accu= 0.6436\n",
      "6 10000 accu= 0.6719\n",
      "6 12000 accu= 0.6661\n",
      "7 2000 accu= 0.6777\n",
      "7 4000 accu= 0.6494\n",
      "7 6000 accu= 0.684\n",
      "7 8000 accu= 0.6804\n",
      "7 10000 accu= 0.6477\n",
      "7 12000 accu= 0.677\n",
      "8 2000 accu= 0.6864\n",
      "8 4000 accu= 0.6885\n",
      "8 6000 accu= 0.6814\n",
      "8 8000 accu= 0.6974\n",
      "8 10000 accu= 0.6939\n",
      "8 12000 accu= 0.6949\n",
      "9 2000 accu= 0.7022\n",
      "9 4000 accu= 0.7059\n",
      "9 6000 accu= 0.6971\n",
      "9 8000 accu= 0.703\n",
      "9 10000 accu= 0.7107\n",
      "9 12000 accu= 0.7126\n",
      "10 2000 accu= 0.7123\n",
      "10 4000 accu= 0.6977\n",
      "10 6000 accu= 0.7069\n",
      "10 8000 accu= 0.7054\n",
      "10 10000 accu= 0.7175\n",
      "10 12000 accu= 0.7216\n",
      "11 2000 accu= 0.7185\n",
      "11 4000 accu= 0.7192\n",
      "11 6000 accu= 0.7199\n",
      "11 8000 accu= 0.7262\n",
      "11 10000 accu= 0.7238\n",
      "11 12000 accu= 0.7177\n",
      "12 2000 accu= 0.723\n",
      "12 4000 accu= 0.7196\n",
      "12 6000 accu= 0.7276\n",
      "12 8000 accu= 0.7228\n",
      "12 10000 accu= 0.7225\n",
      "12 12000 accu= 0.7231\n",
      "13 2000 accu= 0.7273\n",
      "13 4000 accu= 0.7249\n",
      "13 6000 accu= 0.7296\n",
      "13 8000 accu= 0.7238\n",
      "13 10000 accu= 0.7282\n",
      "13 12000 accu= 0.7255\n",
      "14 2000 accu= 0.7332\n",
      "14 4000 accu= 0.7303\n",
      "14 6000 accu= 0.7181\n",
      "14 8000 accu= 0.7259\n",
      "14 10000 accu= 0.7292\n",
      "14 12000 accu= 0.7185\n",
      "15 2000 accu= 0.7259\n",
      "15 4000 accu= 0.7277\n",
      "15 6000 accu= 0.7292\n",
      "15 8000 accu= 0.7156\n",
      "15 10000 accu= 0.7211\n",
      "15 12000 accu= 0.7304\n",
      "16 2000 accu= 0.7317\n",
      "16 4000 accu= 0.7219\n",
      "16 6000 accu= 0.7279\n",
      "16 8000 accu= 0.7196\n",
      "16 10000 accu= 0.7189\n",
      "16 12000 accu= 0.734\n",
      "17 2000 accu= 0.747\n",
      "17 4000 accu= 0.7363\n",
      "17 6000 accu= 0.7235\n",
      "17 8000 accu= 0.7333\n",
      "17 10000 accu= 0.7183\n",
      "17 12000 accu= 0.7288\n",
      "18 2000 accu= 0.7364\n",
      "18 4000 accu= 0.7356\n",
      "18 6000 accu= 0.7308\n",
      "18 8000 accu= 0.722\n",
      "18 10000 accu= 0.7266\n",
      "18 12000 accu= 0.7309\n",
      "19 2000 accu= 0.7279\n",
      "19 4000 accu= 0.7322\n",
      "19 6000 accu= 0.7249\n",
      "19 8000 accu= 0.7317\n",
      "19 10000 accu= 0.7333\n",
      "19 12000 accu= 0.7107\n",
      "20 2000 accu= 0.7408\n",
      "20 4000 accu= 0.7354\n",
      "20 6000 accu= 0.7254\n",
      "20 8000 accu= 0.7211\n",
      "20 10000 accu= 0.7186\n",
      "20 12000 accu= 0.7287\n",
      "21 2000 accu= 0.7319\n",
      "21 4000 accu= 0.7339\n",
      "21 6000 accu= 0.7313\n",
      "21 8000 accu= 0.7283\n",
      "21 10000 accu= 0.7295\n",
      "21 12000 accu= 0.7325\n",
      "22 2000 accu= 0.733\n",
      "22 4000 accu= 0.7238\n",
      "22 6000 accu= 0.7417\n",
      "22 8000 accu= 0.7295\n",
      "22 10000 accu= 0.7242\n",
      "22 12000 accu= 0.7134\n",
      "23 2000 accu= 0.7312\n",
      "23 4000 accu= 0.7212\n",
      "23 6000 accu= 0.7267\n",
      "23 8000 accu= 0.7254\n",
      "23 10000 accu= 0.7267\n",
      "23 12000 accu= 0.7282\n",
      "24 2000 accu= 0.7344\n",
      "24 4000 accu= 0.7402\n",
      "24 6000 accu= 0.7299\n",
      "24 8000 accu= 0.7288\n",
      "24 10000 accu= 0.7354\n",
      "24 12000 accu= 0.7335\n",
      "25 2000 accu= 0.7264\n",
      "25 4000 accu= 0.7406\n",
      "25 6000 accu= 0.7454\n",
      "25 8000 accu= 0.7374\n",
      "25 10000 accu= 0.7399\n",
      "25 12000 accu= 0.7421\n",
      "26 2000 accu= 0.7445\n",
      "26 4000 accu= 0.7462\n",
      "26 6000 accu= 0.7468\n",
      "26 8000 accu= 0.7454\n",
      "26 10000 accu= 0.745\n",
      "26 12000 accu= 0.7456\n",
      "27 2000 accu= 0.7453\n",
      "27 4000 accu= 0.7464\n",
      "27 6000 accu= 0.7435\n",
      "27 8000 accu= 0.7452\n",
      "27 10000 accu= 0.7454\n",
      "27 12000 accu= 0.7462\n",
      "28 2000 accu= 0.7465\n",
      "28 4000 accu= 0.7463\n",
      "28 6000 accu= 0.7458\n",
      "28 8000 accu= 0.7461\n",
      "28 10000 accu= 0.7466\n",
      "28 12000 accu= 0.746\n",
      "29 2000 accu= 0.7477\n",
      "29 4000 accu= 0.7467\n",
      "29 6000 accu= 0.7472\n",
      "29 8000 accu= 0.7466\n",
      "29 10000 accu= 0.7473\n",
      "29 12000 accu= 0.7469\n",
      "30 2000 accu= 0.7458\n",
      "30 4000 accu= 0.7471\n",
      "30 6000 accu= 0.7472\n",
      "30 8000 accu= 0.7474\n",
      "30 10000 accu= 0.7465\n",
      "30 12000 accu= 0.7471\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):   # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n",
    "        #inputs, labels = data 일반 셋팅\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)#GPU 셋팅\n",
    "\n",
    "       \n",
    "        optimizer.zero_grad()#미분값 초기화 안하면 값 누적\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        outputs = net(inputs)# 모델 예측값 저장\n",
    "        loss = criterion(outputs, labels)# 평균 오차\n",
    "        loss.backward()#역전파\n",
    "        optimizer.step()#최적화\n",
    "\n",
    "        #테스트예상 오차비교 출력합니다.\n",
    "     \n",
    "        if i % 2000 == 1999:    # 거의 2000번째마다 작동\n",
    "   \n",
    "            correct = 0\n",
    "            total = 0\n",
    "          \n",
    "            with torch.no_grad(): # no update\n",
    "                for data in test_loader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device)#GPU셋팅\n",
    "                    outputs = net(images)#테스트 모델로 예측값 뽑기\n",
    "                   \n",
    "                    _, predicted = torch.max(outputs.data, 1)#예측값중 가장 높은 값을 답으로 선택\n",
    "                    total += labels.size(0)#전체 데이터 수\n",
    "                    correct += (predicted == labels).sum().item()#정확도 계산\n",
    "                    \n",
    "\n",
    "            print(epoch + 1, i + 1,\"accu=\", correct / total)#정확도  출력\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class airplane is: 80.3 %\n",
      "Accuracy for class automobile is: 84.4 %\n",
      "Accuracy for class bird  is: 62.5 %\n",
      "Accuracy for class cat   is: 58.2 %\n",
      "Accuracy for class deer  is: 70.4 %\n",
      "Accuracy for class dog   is: 64.0 %\n",
      "Accuracy for class frog  is: 81.4 %\n",
      "Accuracy for class horse is: 78.6 %\n",
      "Accuracy for class ship  is: 85.3 %\n",
      "Accuracy for class truck is: 83.1 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "\n",
    "with torch.no_grad():# no update\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음에는 일반 CPU계산으로 돌렸으나 GPU계산이 가능하다는 걸 알고 GPU계산을 이용했는데\n",
    "엄청난 속도차이를 보인다. Conv2d 채널 수를 600개로 실행할 때\n",
    "CPU와 비교해서 압도적으로 빠르게 작동 했다.\n",
    "\n",
    "여러 모델을 사용해봤으나 해당 모델에서 노드와 채널(필터)수만 늘리는게 가장 정확도가 높았다.\n",
    "\n",
    "채널 수는 그대로 두고 신경망 노드 수만 올리면 정확도가 크게 오르지는 않았다.\n",
    "\n",
    "\n",
    "오버피팅이 나기 전까지 동조건에서 채널수와 노드수를 변경하며 최고 정확도를 비교해봤다\n",
    "\n",
    "    (채널수,초기 노드 수)\n",
    "\n",
    "    (6,16)는 59% 최대치 이후 오버 피팅이 났다 (20번 학습)\n",
    "\n",
    "    (60,32)는 68% 최대치 이후 오버 피팅이 났다 (20번 학습)\n",
    "\n",
    "    (600,320)은 73%정도 이후로 오버피팅이 났다 (20번 학습).\n",
    "\n",
    "    (600,320,1200<-히든 노드 수 추가 증가) 74% 이후 오버피팅 (10번 학습) 히든 노드 수를 대량으로 늘리는 건 큰 효과가 없었다\n",
    "    \n",
    "    이 이상으로는 연산속도가 너무 느려서 이 컴퓨터로는 한계인 것 같다\n",
    "\n",
    "채널수와 노드수가 증가할수록 유사한 모델에서 최대 정확도가 증가하는 현상을 보였다.\n",
    "레이어 층수는 어느정도 증가시켜 보았지만 연산 속도가 심하게 느려지는 것 외에는 큰 이득이 없었고\n",
    "최대 정확도는 크게 변하지 않았다.\n",
    "\n",
    "최적화\n",
    "\n",
    "        Adam을 사용해 봤으나 20회 동조건 학습(600,320)에서 67%로 학습속도도 느리고 SGD(73%)보다 정확도가 떨어졌다.\n",
    "        RMSprop을 동조건에서 사용해보았으나 66%가 최대치였다.\n",
    "        SGD사용시 momentum를 0.95로하면 정확도가 떨어진다. 0.8로 두면 0.9와 비슷하고 \n",
    "        0.5로 두면 학습 속도가 떨어졌지만 정확도는 75%이후로 오버피팅이 났다. 30회 정도에서 75% 근처에 도달한다.\n",
    "        정확도가 비교적 정확도가 1~2%정도 높게 나오고 최대 75%까지 올랐는데 단순한 학습속도 원인은 아닌 것으로 보인다.\n",
    "        학습속도를 낮춰 실험해 봤으나 불안정하게 73%까지 도달하기만 하고 75%에 도달하진 못했다. \n",
    "        \n",
    "\n",
    "\n",
    "참고한 코드에서 Kernel Size를 5로 뒀었는데 일반적으로 3으로 두던데 왜 5로 뒀는지 계산해보니\n",
    "\n",
    "    x = self.pool(F.relu(self.conv2(x)))부분에서 Kernel Size를 3으로두면 self.pool에서 1픽셀 손실이 발생한다.\n",
    "\n",
    "    self.pool() 처리에서 (13/2)x(13/2)x4x600=>6x6x320x4\n",
    "\n",
    "    처리과정에서 13/2가 6으로 내려간다.\n",
    "    실제계산에서는 5로 두던 3으로 두던 큰 차이는 없었다. \n",
    "    Kernel Size를 3으로 두면 정보량 증가로 정확도 상승이 있을 것이라 생각했는데\n",
    "    1픽셀 정보 손실이 발생했으나 결과적으로 동조건에서 정확도 최대치 변화가 없었다.\n",
    "    사진 모서리 부분 정보 손실이라 유의미한 데이터를 포함하지 않았던 모양이다.\n",
    "\n",
    "\n",
    "결과적으로 신경망 인풋 정보량이 증가하면 정확도가 상승하는 경향을 보였다.\n",
    "\n",
    "이미지 별로 정확도 예측에서\n",
    "각각의 정확도 예측이 비슷할 줄 알았으나 45%에서~82%정도 차이를 보이는 경우도 있었다.\n",
    "특히나 고양이 대부분의 모델에서 정확도가 매우 낮았다\n",
    "10장 정도의 사진 데이터를 보고 내린 추측이지만\n",
    "\n",
    "정확도가 높은 데이터들의 공통점\n",
    "\n",
    "    1.전체 모습이 보이는 데이터가 많다.\n",
    "    2.색은 달라도 2차원적 형태가 비슷하다. \n",
    "\n",
    "    ex)개구리는 거의 대부분 위에서 찍었고 전체모습이 보인다.\n",
    "    ex)말은 색이 달라도 형태가 비슷한 말이 많았고 대부분 측면을 많이 찍었다.\n",
    " \n",
    "정확도가 낮은 데이터들의 공통점\n",
    "\n",
    "    1.전체 모습이 잘 안보인다. \n",
    "    2.색은 같아도 2차원적 형체가 다르다.\n",
    "\n",
    "    ex)고양이 머리만 찍었거나 뭔가에 가려져 있는 데이터도 있었고 무었보다 데이터를 사방에서 찍었다.\n",
    "    ex)새 타조 참새 닭 등등 형체가 다른게 같은 분류로 구성되어있었고 찍은 구도도 달랐고 타조 머리만 찍힌 것도 있었다.\n",
    "\n",
    "이를 토대로 이 모델에서 예상되는 해결해야 할 문제는 다음과 같다\n",
    "    \n",
    "\n",
    "    1.이미지를 2차원으로 인식하는 문제 고양이 정면과 측면, 후면을 구별 못하는 것 같다. \n",
    "        대상을 입체적으로 판단할 수 있어야한다. \n",
    "        ->이 문제는 대상을 다방면에서 학습시키면 어느정도 해결될 것으로 보인다.\n",
    "        ->완전한 해결을 위해서는 관측 방향을 경우의 수로 나누어 분리해야하는데 그게 어느정도는 가능해보이지만\n",
    "        모든 경우를 나누는 건 불가능해보인다. \n",
    "        \n",
    "    \n",
    "    2.고양이의 관절 움직임 즉 고양이가 다른 자세를 하면 못 알아보는 것 같다.\n",
    "        대상의 관절 동작 가능 범위를 알아야한다.\n",
    "        ->이건 유연한 생물일수록 경우의 수가 증가하는데 3번 문제가 해결되면 해결 가능해보인다\n",
    "    \n",
    "    3.부분적으로 인식을 못하는 문제 고양이 전체와 고양이 머리를 구별 못하는 것 같다.\n",
    "        이미지를 부분적으로 비교할 수 있어야한다.\n",
    "        ->이 문제는 대상을 부분적으로 분리해서 인식해야하는데 이 부분은 데이터 자체를 분할해서 학습시킨뒤\n",
    "        여러 모델을 종류별로 돌려서 이미지에서 일치하는 요소를 확인해야한다.\n",
    "        이 과정은 단순한 과정으로 처리 할 수 없을 것 같다. 단순히 봐도 3차원 회전 문제가 있기 때문에 단순하게 계산하면 요구 연산량이 엄청날 것으로 보인다.\n",
    "        단순한 해법으로는 가능한 경우의 수 전체를 데이터로 입력해 분류 학습하면 가능하지만 그렇게 되면 아주 방대한 양의 데이터가 필요할 것으로 보인다.\n",
    "        \n",
    "결과적으로 새로운 데이터 분석 기술이 나오지 않는한 막대한 양의 데이터 학습 외에는 이 학습 모델에 대한 문제 해결책은 마땅히 없는 것 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
